{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering\n",
    "\n",
    "This notebook is divided into two parts: _building_ the container and _using_ the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Packaging and Uploading of Custom Algorithm for use with Amazon SageMaker\n",
    "\n",
    "### How Amazon SageMaker runs Docker container\n",
    "\n",
    "Amazon SageMaker runs the container with the argument `train` or `serve`. \n",
    "\n",
    "* `ENTRYPOINT` is not defined in the Dockerfile so Docker will run the command `train` at training time and `serve` at serving time. \n",
    "\n",
    "#### Running the container during training\n",
    "\n",
    "When Amazon SageMaker runs training, it uses the `train` script. Files under the `/opt/ml` directory will be used:\n",
    "\n",
    "    /opt/ml\n",
    "    |-- input\n",
    "    |   |-- config\n",
    "    |   |   |-- hyperparameters.json\n",
    "    |   |   `-- resourceConfig.json\n",
    "    |   `-- data\n",
    "    |       `-- <channel_name>\n",
    "    |           `-- <input data>\n",
    "    |-- model\n",
    "    |   `-- <model files>\n",
    "    `-- output\n",
    "        `-- failure\n",
    "\n",
    "##### The input\n",
    "\n",
    "* `/opt/ml/input/config` contains information to control how the program runs. `hyperparameters.json` contains hyperparameter. `resourceConfig.json` describes the network layout used for distributed training. \n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure. \n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>` (for Pipe mode) is the pipe for a given epoch. \n",
    "\n",
    "##### The output\n",
    "\n",
    "* `/opt/ml/model/` stores the model that the custom algorithm generates. This file will be available at the S3 location returned in the `DescribeTrainingJob` result.\n",
    "* `/opt/ml/output` is where the algorithm can write a file `failure` that describes why the job failed. The contents of this file will be returned in the `FailureReason` field of the `DescribeTrainingJob` result. \n",
    "\n",
    "#### Running the container during hosting\n",
    "\n",
    "Here, hosting serves to respond to inference requests that come in via HTTP. The following Python serving stack is used:\n",
    "\n",
    "![Request serving stack](stack.png)\n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` will receive `GET` requests from the infrastructure. The program returns 200 if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these will be passed in as well. \n",
    "\n",
    "The container will have the model files in the same place they were written during training:\n",
    "\n",
    "    /opt/ml\n",
    "    `-- model\n",
    "        `-- <model files>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of the  container\n",
    "\n",
    "In the `container` directory are the components needed to package the custom algorithm for Amazon SageMaker:\n",
    "\n",
    "    .\n",
    "    |-- Dockerfile\n",
    "    |-- build_and_push.sh\n",
    "    `-- log_clustering\n",
    "        |-- nginx.conf\n",
    "        |-- predictor.py\n",
    "        |-- serve\n",
    "        |-- train\n",
    "        `-- wsgi.py\n",
    "\n",
    "* __`Dockerfile`__ describes how to build your Docker container image. More details below.\n",
    "* __`build_and_push.sh`__ is a script that uses the Dockerfile to build the container images and then pushes it to ECR. \n",
    "* __`log_clustering`__ is the directory which contains the files that will be installed in the container.\n",
    "\n",
    "\n",
    "* __`nginx.conf`__ is the configuration file for the nginx front-end. \n",
    "* __`predictor.py`__ is the program that implements the Flask web server and the custom algorithm predictions. \n",
    "* __`serve`__ is the program started when the container is started for hosting. It launches the gunicorn server which runs multiple instances of the Flask app defined in `predictor.py`. \n",
    "* __`train`__ is the program that is invoked when the container is run for training. \n",
    "* __`wsgi.py`__ is a small wrapper used to invoke the Flask app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Build an image that can do training and inference in SageMaker\n",
      "# This is a Python 3 image that uses the nginx, gunicorn, flask stack\n",
      "# for serving inferences in a stable way.\n",
      "\n",
      "FROM ubuntu:18.04\n",
      "\n",
      "MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      "\n",
      "\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
      "    wget \\\n",
      "    python3-pip \\\n",
      "    python3-setuptools \\\n",
      "    nginx \\\n",
      "    ca-certificates \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      "RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
      "\n",
      "# Here we get all python packages.\n",
      "# pip leaves the install caches populated which uses\n",
      "# a significant amount of space. These optimizations save a fair amount of space in the\n",
      "# image, which reduces start up time.\n",
      "RUN python -m pip install --upgrade pip\n",
      "RUN pip --no-cache-dir install Cython\n",
      "RUN pip --no-cache-dir install scikit-learn \n",
      "RUN pip --no-cache-dir install loglizer\n",
      "RUN pip --no-cache-dir install pandas\n",
      "RUN pip --no-cache-dir install numpy\n",
      "RUN pip --no-cache-dir install torchvision\n",
      "RUN pip --no-cache-dir install matplotlib\n",
      "RUN pip --no-cache-dir install flask gunicorn\n",
      "\n",
      "\n",
      "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
      "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
      "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
      "# PATH so that the train and serve programs are found when the container is invoked.\n",
      "\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY agglomerative_clustering /opt/program\n",
      "WORKDIR /opt/program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code builds the container image using `docker build` and push the container image to ECR using `docker push`. \n",
    "\n",
    "This code looks for an ECR repository. If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=agglomerative-clustering\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x agglomerative_clustering/train\n",
    "chmod +x agglomerative_clustering/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Using Custom Algorithm in Amazon SageMaker\n",
    "\n",
    "## Set up the environment\n",
    "\n",
    "A S3 bucket and the role that will be used for working with SageMaker is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = \"DEMO-agglomerative-clustering\"\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session\n",
    "\n",
    "The session is initialized to remember our connection parameters to SageMaker, and used to perform all SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = \"data\"\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator and fit the model\n",
    "\n",
    "In order to use SageMaker to fit/train the custom algorithm, an `Estimator` is created that defines how to use the container to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-03 02:16:46 Starting - Starting the training job...\n",
      "2021-11-03 02:16:53 Starting - Launching requested ML instancesProfilerReport-1635905806: InProgress\n",
      "......\n",
      "2021-11-03 02:18:10 Starting - Preparing the instances for training.........\n",
      "2021-11-03 02:19:39 Downloading - Downloading input data\n",
      "2021-11-03 02:19:39 Training - Downloading the training image.........\n",
      "2021-11-03 02:21:11 Training - Training image download completed. Training in progress.\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34m====== Input data summary ======\u001b[0m\n",
      "\u001b[34mLoading /opt/ml/input/data/training/HDFS_100k.log_structured.csv\u001b[0m\n",
      "\u001b[34m219 94\u001b[0m\n",
      "\u001b[34mTotal: 7940 instances, 313 anomaly, 7627 normal\u001b[0m\n",
      "\u001b[34mTrain: 5557 instances, 219 anomaly, 5338 normal\u001b[0m\n",
      "\u001b[34mTest: 2383 instances, 94 anomaly, 2289 normal\n",
      "\u001b[0m\n",
      "\u001b[34m====== Transformed train data summary ======\u001b[0m\n",
      "\u001b[34mTrain data shape: 5557-by-16\n",
      "\u001b[0m\n",
      "\u001b[34m====== Transformed test data summary ======\u001b[0m\n",
      "\u001b[34mTest data shape: 2383-by-16\n",
      "\u001b[0m\n",
      "\u001b[34m====== Model summary ======\u001b[0m\n",
      "\u001b[34mStarting offline clustering...\u001b[0m\n",
      "\u001b[34mProcessed 1000 instances.\u001b[0m\n",
      "\u001b[34mFound 2 clusters offline.\n",
      "\u001b[0m\n",
      "\u001b[34mStarting online clustering...\u001b[0m\n",
      "\u001b[34mProcessed 2000 instances.\u001b[0m\n",
      "\u001b[34mProcessed 4000 instances.\u001b[0m\n",
      "\u001b[34mProcessed 5338 instances.\u001b[0m\n",
      "\u001b[34mFound 3 clusters online.\n",
      "\u001b[0m\n",
      "\n",
      "2021-11-03 02:24:52 Uploading - Uploading generated training model\n",
      "2021-11-03 02:24:52 Completed - Training job completed\n",
      "\u001b[34mcluster_size_dict: {0: 3999, 1: 1337, 2: 2}\u001b[0m\n",
      "\u001b[34mTrain validation:\u001b[0m\n",
      "\u001b[34m====== Evaluation summary ======\u001b[0m\n",
      "\u001b[34mPrecision: 1.000, recall: 0.434, F1-measure: 0.605\n",
      "\u001b[0m\n",
      "\u001b[34mTest validation:\u001b[0m\n",
      "\u001b[34m====== Evaluation summary ======\u001b[0m\n",
      "\u001b[34mPrecision: 0.950, recall: 0.606, F1-measure: 0.740\n",
      "\u001b[0m\n",
      "\u001b[34mprediction (col)   0.0  1.0\u001b[0m\n",
      "\u001b[34mactual (row)               \u001b[0m\n",
      "\u001b[34m0                 2286    3\u001b[0m\n",
      "\u001b[34m1                   37   57\n",
      "\u001b[0m\n",
      "\u001b[34mRecall:     0.606\u001b[0m\n",
      "\u001b[34mPrecision:  0.950\u001b[0m\n",
      "\u001b[34mF1-measure: 0.740\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "Training seconds: 322\n",
      "Billable seconds: 322\n"
     ]
    }
   ],
   "source": [
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n",
    "image = \"{}.dkr.ecr.{}.amazonaws.com/agglomerative-clustering:latest\".format(account, region)\n",
    "\n",
    "logcluster = sage.estimator.Estimator(\n",
    "    image,\n",
    "    role,\n",
    "    1,\n",
    "    \"ml.c4.2xlarge\",\n",
    "    output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "logcluster.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting trained model\n",
    "The trained model is used to get predictions using HTTP endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = logcluster.deploy(1, \"ml.m4.xlarge\", serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some data and use it for a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.558620e-12</td>\n",
       "      <td>-2.519540e-12</td>\n",
       "      <td>0.041863</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.558620e-12</td>\n",
       "      <td>-2.519540e-12</td>\n",
       "      <td>0.041863</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.558620e-12</td>\n",
       "      <td>-2.519540e-12</td>\n",
       "      <td>0.041863</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1         2         3         4    5    6    7   \\\n",
       "3 -7.558620e-12 -2.519540e-12  0.041863  0.042629  0.042629  0.0  0.0  0.0   \n",
       "0 -7.558620e-12 -2.519540e-12  0.041863  0.042629  0.042629  0.0  0.0  0.0   \n",
       "1 -7.558620e-12 -2.519540e-12  0.041863  0.042629  0.042629  0.0  0.0  0.0   \n",
       "\n",
       "    8    9    10   11   12   13  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv(\"data/payload2.csv\", header=None)\n",
    "test_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_np = test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The csv_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(predictor.predict(test_data_np).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8f6e8c763efa5f3b800e1120943a8aa14818f422933a3d6de94d80de560def5"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
